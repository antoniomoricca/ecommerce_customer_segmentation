# ecommerce_customer_segmentation
Unsupervised learning project to segment e-commerce customers based on behavioral data. Applied feature engineering, K-Means and Hierarchical Clustering, with Elbow Method and Silhouette Score for optimal cluster selection.

# Introduction

Effectively understanding and addressing the diverse needs of customers is a crucial challenge for e-commerce businesses, as it directly impacts marketing efficiency, customer satisfaction, and overall profitability. Without a clear understanding of customer behavior, businesses risk misallocating resources, creating ineffective marketing strategies, and missing opportunities to improve customer retention. To address this challenge, a dataset containing information about e-commerce customers—including details such as location, purchase history, spending habits, and transaction dates—has been provided. The goal of this project is to develop a clustering algorithm to group customers based on shared characteristics. These segments can then be leveraged to optimize marketing efforts, personalize product offerings, and allocate resources more strategically, ultimately fostering a more customer-centric approach and driving business growth.

# Dataset

The dataset contains 951,668 rows, each representing a single purchase. Each row presents these information:

![image](https://github.com/user-attachments/assets/804f8b40-3d0a-47c9-9316-cda10178cd1d)

Before starting the feature engineering, the duplicates and missing rows were deleted.

# Feature Engineering

## Feature Selection

The goal of the project is to segment customers based on their purchasing behavior. Since each row in the dataset represents a single transaction, all purchases made by each customer were aggregated by setting the customer ID as the index. To gain deeper insights into customer patterns, new features were engineered to better capture individual behaviors.
The following features were developed:
1. **Frequency** : Calculated as the total number of purchases made by the customer.
2. **Recency** : Indicates the number of days since the last purchase relative to December 30, 2016, the final date of data acquisition.
3. **Customer Age** : The customer's age as of December 30, 2016.
4. **Product Cost** : The average cost of products purchased by the customer.
5. **Customer Lifetime Value (CLV)**: The total revenue generated by the customer over their lifetime. (calculated as sum of total revenue per each customer)

## Outliers Removal

To remove outliers, I used Isolation Forest, a machine learning algorithm based on the concept of isolating data points in a feature space. The algorithm works by building random decision trees and identifying points that are isolated with fewer splits as potential outliers. Several parameters were tested, and the following were chosen:
- Number of estimators: 200, a robust value that provided good performance without excessive training time.
Confidential C
- Contamination: 0.05, indicating that approximately 5% of the data was expected to be outliers.

With these parameters, 3,415 outliers were identified and consequently removed from the dataset, representing about 5%.

# Clustering

## Optimal number of clusters

After selecting the features, it was necessary to scale them for the k-means algorithm. For this purpose, Standard Scaling was chosen, as it is less sensitive to outliers previously identified. 

Subsequently, two differnt methods were used to choiche the optimal number of clusters:

1. **Elbow Method** : This method involves running the k-means algorithm for a range of values of the number of clusters (k). For each value of k, the WCSS (Within-Cluster Sum of Squares) is calculated, which represents the sum of squared distances between the points and the centroid of their cluster.
2. **Silhouette Score** : In this method, multiple clusters are also created by varying the number of clusters. The silhouette score evaluates the relationship between intra-cluster distances and inter-cluster distances. A high score indicates that the clusters are well-defined and separated, whereas a low score suggests overlapping or poorly defined clusters.

![image](https://github.com/user-attachments/assets/982a754c-ca89-4da2-b16e-14d92c106c56)

Regarding the Elbow Method plot, there doesn't seem to be a clear indication of the "elbow," which would suggest the optimal number of clusters. However, it can be observed that the curve's slope appears to decrease between 4 and 6 clusters.

The Silhouette Method, on the other hand, indicates that the number of clusters that maximizes the score is 2. In this case, the choice of 2 clusters was discarded, as it would lead to a loss of information due to less differentiation between clusters. With only two clusters, the model would fail to capture the underlying complexities in the data. Therefore, the number of clusters, k, was chosen as the one with the second-highest score, which is 5.

Before definitively determining the optimal number of clusters, an additional check was performed by evaluating the silhouette score plots for each point within the clusters. In this case, alongside the average silhouette score, any negative values were also considered, which can indicate that some points might be closer to points in neighboring clusters than to their own cluster's centroid. This suggests that those points may not be well-defined or may have been misclassified, implying a potential overlap between clusters.

From the analysis of all the plots, it appears that as the number of clusters changes, there
are always points with negative silhouette scores. Therefore, based on this analysis, it does
not seem that one number of clusters is clearly better than another.

![image](https://github.com/user-attachments/assets/abc42334-26af-479d-9a08-b990c2b035b3)

## K-means Clustering

K-means clustering is used to segment the customers in the dataset. It is an unsupervised machine learning algoritmh which minimizes intra cluster
variance to group data points. The initialization of the centroids has been done using k means++, which ensures a well spread distribution of them.

The algorithm generated 5 clusters, with a balanced distribution of customers among them. To visualize the clusters in 2D, Principal Component Analysis (PCA) was applied, and the corresponding plot is shown:

![image](https://github.com/user-attachments/assets/1b6d9673-f38c-4eb6-9519-f0694d7a6df3)

From the plot, it can be observed that some clusters overlap. This is because the original clusters were formed in a 5 dimensional space, and reducing the dimensionality to 2D may not fully capture the underlying structure of the data. The Xs in the grap h represent the centroids.

To properly interpret the clusters, boxplots were created to examine the characteristics of each group in relation to the features used in the model:

![image](https://github.com/user-attachments/assets/2c2f7cfc-0ba9-487b-9d0f-9cd94c8557e7)

The same characteristic can be visualized by looking at the centroid’s position s in the features space:

![image](https://github.com/user-attachments/assets/57b59b3d-be02-4e78-a27e-c5737181d833)

## Hierachical Clustering

From the dendrogram, it appears that the optimal number of clusters is 2, 4, or 5, as there is a noticeable jump in the distance at these points, indicating a clear separation between the clusters . Based on the previous analysis, the number of clusters was set to 5 as the optimal choice for performing the agglomerative clustering algorithm.

![image](https://github.com/user-attachments/assets/0c562849-7d3d-41e1-88d9-064b0a30fcb1)

# Conclusion

In this project, a clustering algorithm was developed to segment e-commerce customers, aiming to enhance marketing efforts. Specifically, a k means clustering approach was implemented, dividing the 68,300 unique customers in the dataset into 5 distinct gro ups. The group characteristics are summarized in the table below:

![image](https://github.com/user-attachments/assets/79f06a10-6e5c-496a-a411-7f110b40ccac)






